\section{Discussion}

This paper has proposed a new framework for deriving optimisation algorithms for non-convex composite objective functions, which are particularly prevalent in the field of machine learning and the subfield of deep learning. What we have proposed is truly a \textit{framework}: it can be applied to a new loss function by writing down its Bregman divergence, or a new machine learning model by writing down its architectural perturbation bound. The framework is properly placed in the context of existing frameworks such as the majorise-minimise meta-algorithm, mirror descent and natural gradient descent.

Recent papers have proposed a paradigm of \textit{hyperparameter transfer} where a small network is tuned and the resulting hyperparameters are transferred to a larger network \citep{yang2021tuning, bernstein-thesis}. The methods and results in this paper suggest a stronger paradigm of \textit{hyperparameter elimination}: by detailed analysis of the structure and interactions between different components of a machine learning system, we may hope---if not to outright outlaw hyperparameters---at least to reduce their abundance and opacity.

The main product of this research is automatic gradient descent (AGD), with pseudocode given in \cref{alg:agd} and PyTorch code given in \cref{app:pytorch}. We have found AGD to be genuinely useful, and believe that it may complement automatic differentiation in helping to automate general machine learning workflows.

The analysis leading to automatic gradient descent is elementary: we leverage basic concepts in linear algebra such as matrix and vector norms, and use simple bounds such as the triangle inequality for vector--vector sums, and the operator norm bound for matrix--vector products. The analysis is non-asymptotic: it does not rely on taking dimensions to infinity, and deterministic: it does not involve random matrix theory. We believe that the accessibility of the analysis could make this paper a good starting point for future developments.

\paragraph{Directions for future work} Here we list some promising avenues for theoretical and practical research. We are exploring some of these ideas in our development codebase: \url{https://github.com/C1510/agd_exp}.

\begin{itemize}[leftmargin=*]
    \item \textit{Stochastic optimisation.} Automatic gradient descent is derived in the full-batch optimisation setting, but the algorithm is evaluated experimentally in the mini-batch setting. It would be interesting to try to extend our theoretical and practical methods to more faithfully address stochastic optimisation.
    \item \textit{More architectures.} Automatic gradient descent is derived for fully-connected networks and extended heuristically to convolutional networks. We are curious to extend the methods to more varied architectures such as transformers \citep{NIPS2017_3f5ee243} and architectural components such as biases. Since most neural networks resemble fully-connected networks in the sense that they are all just deep compound operators, we expect much of the structure of automatic gradient descent as presented to carry through.
    \item \textit{Regularisation.} The present paper deals purely with the optimisation structure of deep neural networks, and little thought is given to either generalisation or regularisation. Future work could look at both theoretical and practical regularisation schemes for automatic gradient descent. It would be interesting to try to do this without introducing hyperparameters, although we suspect that when it comes to regularisation at least one hyperparameter may become necessary.
    \item \textit{Acceleration.} We have found in some preliminary experiments that slightly increasing the update size of automatic gradient descent with a gain hyperparameter, or introducing a momentum hyperparameter, can lead to faster convergence. We emphasise that no experiment in this paper used such hyperparameters. Still, these observations may provide a valuable starting point for improving AGD in future work.
    \item \textit{Operator perturbation theory.} Part of the inspiration for this paper was the idea of applying operator perturbation theory to deep learning. While perturbation theory is well-studied in the context of linear operators \citep{Weyl1912,Kato:1966:PTL,STEWART200653}, in deep learning we are concerned with non-linear compound operators. It may be interesting to try to further extend results in perturbation theory to deep neural networks. One could imagine cataloging the perturbation structure of different neural network building blocks, and using a result similar to deep relative trust (\cref{lem:deep_perturbation_bounds}) to describe how they compound.
\end{itemize}