\begin{algorithm}[t]
\caption{\captiontitle{Automatic gradient descent.} The matrix $\mW_k$ in $\R^{d_k \times d_{k-1}}$ is the weight matrix at layer $k$. The gradient $\nabla_{\mW_k} \el$ is with respect to the objective $\el$ evaluated on a mini-batch $B$ of training samples.}\label{alg:agd}
\begin{algorithmic}
\tt
\setstretch{1.8}\vspace{0.5em}
\DEF[initialise\_weights]
\FOR{layer $k$ in $\{1,...,L\}$:}
\STATE $\mW_k \sim \uniform(\mathtt{orthogonal}(d_k,d_{k-1}))$ \WCOMMENT{sample a semi-orthogonal matrix}
\STATE $\mW_k \gets \mW_k \cdot \sqrt{\frac{d_k}{d_{k-1}}}$ \WCOMMENT{rescale its singular values}
\ENDFOR
\ENDDEF
    \vspace{-1.6ex}\DEF[update\_weights]
    \STATE $G \gets \frac{1}{L}\sum_{l=1}^L \norm{\nabla_{\mW_k} \el}_F \cdot \sqrt{\frac{d_k}{d_{k-1}}}$ \WCOMMENT{get gradient summary}
\STATE $\smash{\eta \gets \log\frac{1 + \sqrt{1+ 4G }}{2}}$ \WCOMMENT{set automatic learning rate}
\FOR{layer $k$ in $\{1,...,L\}$:}
    \STATE $\mW_k \gets \mW_k - \frac{\eta}{L} \cdot \frac{\nabla_{\mW_k} \el}{\norm{\nabla_{\mW_k} \el}_F} \cdot \sqrt{\frac{d_k}{d_{k-1}}}$ \WCOMMENT{update weights}
\ENDFOR
\ENDDEF
\setstretch{1.0}
\end{algorithmic}
\end{algorithm}